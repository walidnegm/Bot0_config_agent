import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM


class LLMManager:
    def __init__(self, use_openai: bool = False):
        self.use_openai = use_openai

        if self.use_openai:
            print("[LLMManager] âš ï¸ Skipping local model load â€” using OpenAI backend.")
            self.tokenizer = None
            self.model = None
            self.eos = None
            return

        try:
            print("[LLMManager] ðŸ” Locating local LLaMA modelâ€¦")

            model_root = Path.home() / "projects/Bot0_config_agent/model"
            snapshot_base = model_root / "models--meta-llama--Meta-Llama-3-8B-Instruct" / "snapshots"
            candidates = list(snapshot_base.glob("*"))

            if not candidates:
                raise FileNotFoundError(f"No model snapshot found under: {snapshot_base}")
            
            model_path = candidates[0]
            print(f"[LLMManager] âœ… Using model path: {model_path}")

            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                use_fast=False,
                local_files_only=True
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="auto",
                torch_dtype=torch.float16,
                local_files_only=True
            )

            self.eos = self.tokenizer.eos_token_id
            print("[LLMManager] ðŸš€ Model loaded successfully.")

        except Exception as e:
            print(f"âŒ [LLMManager] Failed to load model: {e}")
            raise

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 256,
        temperature: float = 0.0,
        system_prompt: str = None
    ) -> str:
        if self.use_openai:
            raise RuntimeError("LLMManager is in OpenAI mode â€” local generation is disabled.")

        full_prompt = (
            f"[SYSTEM] {system_prompt}\n[USER] {prompt}\n[ASSISTANT]"
            if system_prompt else prompt
        )

        print(f"[LLMManager] Full prompt:\n{repr(full_prompt)}\n")

        try:
            inputs = self.tokenizer(full_prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=temperature > 0.0,
                    temperature=temperature,
                    pad_token_id=self.eos
                )

            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

            print(f"[LLMManager] Full decoded output:\n{repr(full_text)}\n")

            if full_text.startswith(full_prompt):
                generated_text = full_text[len(full_prompt):].strip()
            else:
                print("âš ï¸ [LLMManager] Prompt prefix not found. Returning full decoded text.")
                generated_text = full_text

            print(f"[LLMManager] ðŸ§ª Generated text before return:\n{repr(generated_text)}\n")

            if not isinstance(generated_text, str):
                raise ValueError(f"Generated output is not a string: {type(generated_text)}")

            return generated_text

        except Exception as e:
            print(f"âŒ [LLMManager] Generation failed: {e}")
            raise


# Shared instance
_llm = LLMManager()

def ask_llm(prompt: str, temperature: float = 0.0, role: str = None) -> str:
    system_prompt = {
        "copilot": "You are GitHub Copilot, a helpful developer assistant.",
        "ops": "You are an infrastructure and DevOps expert.",
        "qa": "You are a strict QA tester reviewing behavior and stability.",
        "helper": "You are a friendly and clear technical assistant."
    }.get(role, "You are a helpful assistant.")

    return _llm.generate(
        prompt=prompt,
        temperature=temperature,
        system_prompt=system_prompt
    )

